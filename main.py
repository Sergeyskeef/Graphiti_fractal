import argparse
import asyncio
from datetime import datetime, timezone
import sys
from pathlib import Path

# Apply library patches before importing core logic
sys.path.append(str(Path(__file__).parent / "scripts"))
try:
    from apply_patches import apply_patches
    apply_patches()
except ImportError:
    pass

from core import (
    DecisionEntity,
    ProjectEntity,
    TeamEntity,
    TechnicalConceptEntity,
    L3Summary, # –î–æ–±–∞–≤–ª–µ–Ω–æ
    get_graphiti_client,
)
from core.graphiti_client import get_write_semaphore
from queries.context_builder import build_agent_context
from queries.quality_check import check_graph_quality
from queries.search_strategies import test_search_strategies
from layers.l1_consolidation import get_l1_context
from layers.l2_semantic import get_l2_semantic_context
from layers.l3_fractal import get_l3_fractal_context
from visualization.visualization_export import export_to_file
from benchmarks.benchmark import run_benchmark
from core.migrations import apply_migrations
from scripts.consolidate import run_consolidate as cmd_consolidate # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Å–∫—Ä–∏–ø—Ç –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏
from queries.dedupe_entities import main as dedupe_entities_main


async def ensure_graphiti():
    client = get_graphiti_client()
    graphiti = await client.ensure_ready()
    return graphiti


async def cmd_setup(args):
    graphiti = await ensure_graphiti()
    mig = await apply_migrations(graphiti)
    print("‚úÖ Graphiti/Neo4j –≥–æ—Ç–æ–≤, –∏–Ω–¥–µ–∫—Å—ã —Å–æ–∑–¥–∞–Ω—ã")
    if mig["total"] > 0:
        print(f"‚úÖ Migrations: applied={mig['applied']} skipped={mig['skipped']} total={mig['total']}")


async def cmd_seed(args):
    graphiti = await ensure_graphiti()
    episodes = [
        dict(
            name="Project Overview",
            body="""
            Sergey and Natasha are working on a Fractal Memory project.

            The project has three main components:
            1. Graph Engine - built with Neo4j for knowledge representation
            2. LLM Integration - using GPT-4 for entity extraction and reasoning
            3. Temporal Processing - maintaining bi-temporal data (valid_from, valid_to)

            The project status is in Development phase.
            Sergey is the primary developer.
            Priority is High - this is a core research initiative.

            Key concepts involved:
            - Fractal Architecture: a hierarchical representation system
            - Knowledge Graph: semantic network of entities and relationships
            - Temporal Logic: maintaining contradictions over time

            These concepts are at Advanced abstraction level (3-4).
            """,
            source="project_documentation",
        ),
        dict(
            name="Strategic Decision - Vanilla First",
            body="""
            Decision made on 2025-12-10:

            "We decided to simplify the Fractal Memory implementation by starting with 
            vanilla Graphiti instead of building custom Redis buffer layer."

            Made by: Natasha
            Rationale: Reduce complexity, avoid Integration Hell, focus on core value.
            Dependencies: This affects L0 optimization, L1 consolidation logic.
            Status: Active - this is our current strategy.
            """,
            source="decision_log",
        ),
        dict(
            name="Team Structure",
            body="""
            The development team consists of:
            - Sergey: Senior Developer, specializing in AI/ML and Python
            - Natasha: Technical Lead and Business Advisor, strategic guidance

            Team Name: Fractal Memory Core Team
            Focus: Building production-grade memory system for AI agents
            Communication: Primarily Telegram for async discussions
            """,
            source="team_documentation",
        ),
    ]

    write_semaphore = get_write_semaphore()
    for ep in episodes:
        async with write_semaphore:
            await graphiti.add_episode(
                name=ep["name"],
                episode_body=ep["body"],
                source_description=ep["source"],
                reference_time=datetime.now(timezone.utc),
            )
        print(f"‚úÖ Added episode: {ep['name']}")

    # Link user to person entity
    from knowledge.ingest import link_user_to_person_entity
    await link_user_to_person_entity(graphiti, "sergey", "Sergey")

    if args.with_search:
        await cmd_search_demo(args)

    print("‚ú® Demo data loaded")


async def cmd_clear(args):
    graphiti = await ensure_graphiti()
    # –ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –≥—Ä–∞—Ñ–∞
    await graphiti.driver.execute_query("MATCH (n) DETACH DELETE n")
    await graphiti.build_indices_and_constraints()
    await apply_migrations(graphiti)
    print("üßπ Graph cleared and indices recreated")


async def cmd_migrate(args):
    graphiti = await ensure_graphiti()
    mig = await apply_migrations(graphiti)
    print(f"‚úÖ Migrations: applied={mig['applied']} skipped={mig['skipped']} total={mig['total']}")


async def cmd_quality(args):
    await check_graph_quality()


async def cmd_search_demo(args):
    await test_search_strategies()


async def cmd_context(args):
    graphiti = await ensure_graphiti()
    context = await build_agent_context(
        graphiti,
        entity_name=args.entity,
        context_size=args.size,
    )
    if context:
        print(context)
    else:
        print("‚ö†Ô∏è  –°—É—â–Ω–æ—Å—Ç—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")


async def cmd_l1(args):
    graphiti = await ensure_graphiti()
    summary = await get_l1_context(
        graphiti,
        user_context=args.query,
        hours_back=args.hours,
    )
    print(summary)


async def cmd_l2(args):
    graphiti = await ensure_graphiti()
    summary = await get_l2_semantic_context(graphiti, args.entity)
    if summary:
        print(summary)
    else:
        print("‚ö†Ô∏è  –°—É—â–Ω–æ—Å—Ç—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")


async def cmd_l3(args):
    graphiti = await ensure_graphiti()
    summary = await get_l3_fractal_context(graphiti, args.entity)
    if summary:
        print(summary)
    else:
        print("‚ö†Ô∏è  –°—É—â–Ω–æ—Å—Ç—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")


async def cmd_viz_export(args):
    graphiti = await ensure_graphiti()
    await export_to_file(graphiti, filename=args.output)


async def cmd_benchmark(args):
    await run_benchmark()


def build_parser():
    parser = argparse.ArgumentParser(description="Fractal Memory / Graphiti CLI")
    subparsers = parser.add_subparsers(dest="command", required=True)

    subparsers.add_parser("setup", help="–°–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å—ã/–∫–æ–Ω—Å—Ç—Ä–µ–π–Ω—Ç—ã Graphiti").set_defaults(
        func=cmd_setup
    )

    seed_p = subparsers.add_parser("seed", help="–°–æ–∑–¥–∞—Ç—å –¥–µ–º–æ-—ç–ø–∏–∑–æ–¥—ã –∏ —Å—É—â–Ω–æ—Å—Ç–∏")
    seed_p.add_argument("--with-search", action="store_true", help="–ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–æ–∏—Å–∫–æ–≤—É—é –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—é –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏")
    seed_p.set_defaults(func=cmd_seed)

    subparsers.add_parser("quality", help="–û—Ç—á—ë—Ç –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –≥—Ä–∞—Ñ–∞").set_defaults(func=cmd_quality)
    subparsers.add_parser("clear", help="–û—á–∏—Å—Ç–∏—Ç—å –≥—Ä–∞—Ñ (reset) –∏ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å—ã").set_defaults(func=cmd_clear)
    subparsers.add_parser("migrate", help="–ü—Ä–∏–º–µ–Ω–∏—Ç—å –º–∏–≥—Ä–∞—Ü–∏–∏ –∏–∑ –ø–∞–ø–∫–∏ migrations/").set_defaults(func=cmd_migrate)
    subparsers.add_parser("search-demo", help="–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –ø–æ–∏—Å–∫–∞").set_defaults(func=cmd_search_demo)

    ctx_p = subparsers.add_parser("context", help="–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Å—É—â–Ω–æ—Å—Ç–∏")
    ctx_p.add_argument("entity", help="–ò–º—è —Å—É—â–Ω–æ—Å—Ç–∏")
    ctx_p.add_argument("--size", choices=["minimal", "medium", "full"], default="full")
    ctx_p.set_defaults(func=cmd_context)

    l1_p = subparsers.add_parser("l1", help="L1 recent context summary")
    l1_p.add_argument("--query", default="Fractal Memory", help="–ü–æ–∏—Å–∫–æ–≤–∞—è —Ñ—Ä–∞–∑–∞ –¥–ª—è –Ω–µ–¥–∞–≤–Ω–∏—Ö —ç–ø–∏–∑–æ–¥–æ–≤")
    l1_p.add_argument("--hours", type=int, default=24, help="–ß–∞—Å–æ–≤ –Ω–∞–∑–∞–¥ –¥–ª—è –≤—ã–±–æ—Ä–∫–∏")
    l1_p.set_defaults(func=cmd_l1)

    l2_p = subparsers.add_parser("l2", help="L2 semantic patterns")
    l2_p.add_argument("entity", help="–ò–º—è —Å—É—â–Ω–æ—Å—Ç–∏")
    l2_p.set_defaults(func=cmd_l2)

    l3_p = subparsers.add_parser("l3", help="L3 fractal abstraction")
    l3_p.add_argument("entity", help="–ò–º—è —Å—É—â–Ω–æ—Å—Ç–∏")
    l3_p.set_defaults(func=cmd_l3)

    viz_p = subparsers.add_parser("viz-export", help="–≠–∫—Å–ø–æ—Ä—Ç –≥—Ä–∞—Ñ–∞ –≤ JSON –¥–ª—è D3")
    viz_p.add_argument(
        "--output",
        default="visualization/graph_data.json",
        help="–ü—É—Ç—å –∫ JSON (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é visualization/graph_data.json)",
    )
    viz_p.set_defaults(func=cmd_viz_export)

    subparsers.add_parser("benchmark", help="–ü–µ—Ä—Ñ–æ–º–∞–Ω—Å add_episode/search").set_defaults(func=cmd_benchmark)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–º–∞–Ω–¥—É –¥–ª—è –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏ L3
    consolidate_p = subparsers.add_parser("consolidate", help="–ó–∞–ø—É—Å—Ç–∏—Ç—å –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—é L3 –ø–∞–º—è—Ç–∏")
    consolidate_p.add_argument("--hours", type=int, default=24*7, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞—Å–æ–≤ –¥–ª—è –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏")
    consolidate_p.set_defaults(func=lambda args: cmd_consolidate(ensure_graphiti(), hours_back=args.hours))

    # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–º–∞–Ω–¥—É –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ Entity —É–∑–ª–æ–≤
    dedupe_p = subparsers.add_parser("dedupe-entities", help="–î–µ–¥—É–ø–ª–∏—Ü–∏—Ä–æ–≤–∞—Ç—å Entity —É–∑–ª—ã –ø–æ –∏–º–µ–Ω–∏")
    dedupe_p.add_argument("--dry-run", action="store_true", help="–ê–Ω–∞–ª–∏–∑ –±–µ–∑ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π")
    dedupe_p.set_defaults(func=lambda args: dedupe_entities_main(dry_run=args.dry_run))


    return parser


def main():
    parser = build_parser()
    args = parser.parse_args()
    asyncio.run(args.func(args))


if __name__ == "__main__":
    main()

